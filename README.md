# Wine Quality Classification

## Problem Statement

This project aims to classify the quality of red wine as 'good' or 'bad' based on its physicochemical properties. This is a binary classification problem.

## Dataset Description

The dataset used is the Wine Quality dataset from the UCI Machine Learning Repository. It contains 1599 instances of red wine, each with 11 physicochemical properties (e.g., fixed acidity, volatile acidity, citric acid, etc.) and a quality score from 0 to 10. For this project, the quality score is converted to a binary variable where wines with a score of 7 or higher are considered 'good' and the rest are 'bad'.

## Models Used

The following classification models were implemented:

1.  Logistic Regression
2.  Decision Tree Classifier
3.  K-Nearest Neighbor Classifier
4.  Naive Bayes Classifier
5.  Random Forest
6.  XGBoost

### Model Performance Comparison

**Note to user:** Please run the Streamlit app, select each model, and fill in the following table with the metrics displayed in the app.

| ML Model Name     | Accuracy | AUC  | Precision | Recall | F1 Score | MCC  |
| ----------------- | -------- | ---- | --------- | ------ | -------- | ---- |
| Logistic Regression |          |      |           |        |          |      |
| Decision Tree     |          |      |           |        |          |      |
| kNN               |          |      |           |        |          |      |
| Naive Bayes       |          |      |           |        |          |      |
| Random Forest     |          |      |           |        |          |      |
| XGBoost           |          |      |           |        |          |      |

### Observations on Model Performance

**Note to user:** Please add your observations on the performance of each model on the chosen dataset in the table below.

| ML Model Name     | Observation about model performance |
| ----------------- | ----------------------------------- |
| Logistic Regression |                                     |
| Decision Tree     |                                     |
| kNN               |                                     |
| Naive Bayes       |                                     |
| Random Forest     |                                     |
| XGBoost           |                                     |